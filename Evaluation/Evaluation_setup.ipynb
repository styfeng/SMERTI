{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation_Setup.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBppl_3hPEJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfKNIlit-uEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io, json, os, collections, pprint, time\n",
        "import re\n",
        "from string import punctuation\n",
        "import unicodedata\n",
        "import random\n",
        "import operator\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_CN5Y69PMCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install stanfordcorenlp\n",
        "from string import punctuation\n",
        "from collections import OrderedDict\n",
        "\n",
        "import nltk\n",
        "from nltk.tree import *\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "nlp = StanfordCoreNLP(r'/content/drive/My Drive/stanford-corenlp-full-2018-10-05')\n",
        "\n",
        "\n",
        "def extract_leaves(tree):\n",
        "    leaves_list = []\n",
        "    for i in tree.subtrees():\n",
        "        if i.label() in [\"NN\", \"NNS\", \"NP\"]:\n",
        "            leaves_list.append(i.leaves())\n",
        "    return leaves_list\n",
        "\n",
        "\n",
        "def create_entities_list(list_of_lists):\n",
        "    entities_list = []\n",
        "    for entity_list in list_of_lists:\n",
        "        entity = ' '.join(entity_list).replace(\" '\", \"'\")\n",
        "        if entity not in entities_list:\n",
        "          entities_list.append(entity)\n",
        "    return entities_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZCm6WzDPjgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_trees(initial_sentence):\n",
        "  full_leaves_list = []\n",
        "  sentences = re.split('[?.!]', initial_sentence)\n",
        "  sentences = list(filter(lambda x: x not in ['',' '], sentences))\n",
        "  for sentence in sentences:\n",
        "    sentence = Tree.fromstring(nlp.parse(sentence))\n",
        "    leaves = extract_leaves(sentence)\n",
        "    full_leaves_list = full_leaves_list + leaves\n",
        "  clean_leaves_list = create_entities_list(full_leaves_list)\n",
        "  return clean_leaves_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfy7H7ljNv5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lines(path):\n",
        "  lines_list = []\n",
        "  file = io.open(path, encoding = 'utf-8')\n",
        "  for line in file:\n",
        "    clean_line = re.sub('\\s+', ' ', line).strip()\n",
        "    lines_list.append(clean_line)\n",
        "  return lines_list\n",
        "\n",
        "\n",
        "def get_sent_lines(path, sent):\n",
        "  sent_lines = []\n",
        "  file = io.open(path, encoding = 'utf-8')\n",
        "  for line in file:\n",
        "    clean_line = re.sub('\\s+', ' ', line).strip()\n",
        "    sent_line = [clean_line, sent]\n",
        "    sent_lines.append(sent_line)\n",
        "  return sent_lines\n",
        "\n",
        "\n",
        "def get_nouns(full_list):\n",
        "  nouns_dict = {}\n",
        "  counter = 0\n",
        "  for line in full_list:\n",
        "    if counter % 1000 == 0:\n",
        "      print(\"Got nouns for {} lines.\".format(counter))\n",
        "    line_nouns = get_trees(line)\n",
        "    for noun in line_nouns:\n",
        "      if noun in nouns_dict:\n",
        "        nouns_dict[noun] += 1\n",
        "      else:\n",
        "        nouns_dict[noun] = 1\n",
        "    counter += 1\n",
        "  return nouns_dict\n",
        "\n",
        "\n",
        "def get_sorted_list(nouns_dict):\n",
        "  sorted_list = sorted(nouns_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  return sorted_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RBjnoyeVUjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_lines(pos_list, neg_list, test_nouns):\n",
        "  temp_test_lines_1 = {}\n",
        "  temp_test_lines_2 = {}\n",
        "  temp_test_lines_3 = {}\n",
        "  \n",
        "  final_test_lines_1 = []\n",
        "  final_test_lines_2 = []\n",
        "  final_test_lines_3 = []\n",
        "  \n",
        "  for noun in test_nouns:\n",
        "    noun_lines_pos = list(filter(lambda x: noun not in x[0] and len(x[0].split()) > 5, pos_list))\n",
        "    noun_lines_neg = list(filter(lambda x: noun not in x[0] and len(x[0].split()) > 5, neg_list))\n",
        "    \n",
        "    random.seed(3)\n",
        "    random.shuffle(noun_lines_pos)\n",
        "    random.shuffle(noun_lines_neg)\n",
        "    \n",
        "    noun_lines_pos_1 = noun_lines_pos[:50]\n",
        "    noun_lines_neg_1 = noun_lines_neg[:50] \n",
        "    noun_lines_full_1 =  noun_lines_pos_1 + noun_lines_neg_1\n",
        "        \n",
        "    noun_lines_pos_2 = noun_lines_pos[50:100]\n",
        "    noun_lines_neg_2 = noun_lines_neg[50:100] \n",
        "    noun_lines_full_2 = noun_lines_pos_2 + noun_lines_neg_2\n",
        "        \n",
        "    noun_lines_pos_3 = noun_lines_pos[100:150]\n",
        "    noun_lines_neg_3 = noun_lines_neg[100:150]\n",
        "    noun_lines_full_3 = noun_lines_pos_3 + noun_lines_neg_3\n",
        "    \n",
        "    temp_test_lines_1[noun] = noun_lines_full_1\n",
        "    temp_test_lines_2[noun] = noun_lines_full_2\n",
        "    temp_test_lines_3[noun] = noun_lines_full_3\n",
        "\n",
        "  for noun in test_nouns:\n",
        "    for line in temp_test_lines_1[noun]:\n",
        "      final_test_lines_1.append([noun] + line)\n",
        "    for line in temp_test_lines_2[noun]:\n",
        "      final_test_lines_2.append([noun] + line)\n",
        "    for line in temp_test_lines_3[noun]:\n",
        "      final_test_lines_3.append([noun] + line)\n",
        "  \n",
        "  return final_test_lines_1, final_test_lines_2, final_test_lines_3\n",
        "\n",
        "\n",
        "def get_news_lines(full_list, test_nouns):\n",
        "  temp_news_lines_1 = {}\n",
        "  temp_news_lines_2 = {}\n",
        "  temp_news_lines_3 = {}\n",
        "  \n",
        "  final_news_lines_1 = []\n",
        "  final_news_lines_2 = []\n",
        "  final_news_lines_3 = []\n",
        "  \n",
        "  for noun in test_nouns:\n",
        "    noun_lines = list(filter(lambda x: noun not in x and len(x.split()) > 5, full_list))\n",
        "    \n",
        "    random.seed(3)\n",
        "    random.shuffle(noun_lines)\n",
        "    \n",
        "    noun_lines_1 = noun_lines[:100]\n",
        "    noun_lines_2 = noun_lines[100:200] \n",
        "    noun_lines_3 = noun_lines[200:300]\n",
        "    \n",
        "    temp_news_lines_1[noun] = noun_lines_1\n",
        "    temp_news_lines_2[noun] = noun_lines_2\n",
        "    temp_news_lines_3[noun] = noun_lines_3\n",
        "\n",
        "  for noun in test_nouns:\n",
        "    for line in temp_news_lines_1[noun]:\n",
        "      final_news_lines_1.append(noun + '\\t' + line)\n",
        "    for line in temp_news_lines_2[noun]:\n",
        "      final_news_lines_2.append(noun + '\\t' + line)\n",
        "    for line in temp_news_lines_3[noun]:\n",
        "      final_news_lines_3.append(noun + '\\t' + line)\n",
        "  \n",
        "  return final_news_lines_1, final_news_lines_2, final_news_lines_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRpVlOkpgp48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_nouns(nouns_list, path):\n",
        "  f = io.open(path, \"w\", encoding = 'utf-8')\n",
        "  print(\"Currently writing nouns to file ...\")\n",
        "  f.write('\\n'.join('{}\\t{}'.format(x[0],x[1]) for x in nouns_list))\n",
        "  f.close()\n",
        "  print(\"Nouns successfully written to file!\")  \n",
        "\n",
        "  \n",
        "def write_eval_lines(final_test_lines, path):\n",
        "  f = io.open(path, \"w\", encoding = 'utf-8')\n",
        "  print(\"Currently writing evaluation lines to file ...\")\n",
        "  f.write('\\n'.join('{}\\t{}\\t{}'.format(x[0],x[1],x[2]) for x in final_test_lines))\n",
        "  f.close()\n",
        "  print(\"Evaluation lines successfully written to file!\")\n",
        "\n",
        "  \n",
        "def write_news_lines(final_news_lines, path):\n",
        "  f = io.open(path, \"w\", encoding = 'utf-8')\n",
        "  print(\"Currently writing news evaluation headlines to file ...\")\n",
        "  f.write('\\n'.join(x for x in final_news_lines))\n",
        "  f.close()\n",
        "  print(\"News evaluation headlines successfully written to file!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBBmvXEHJHzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_pos = '/content/drive/My Drive/Yelp_Dataset/data/seq2seq_data/test_reviews_positive.txt'\n",
        "path_neg = '/content/drive/My Drive/Yelp_Dataset/data/seq2seq_data/test_reviews_negative.txt'\n",
        "path_full = '/content/drive/My Drive/Yelp_Dataset/data/seq2seq_data/final_test_reviews.txt'\n",
        "\n",
        "#path_eval_1 = '/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/eval_reviews_6_(1).txt'\n",
        "#path_eval_2 = '/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/eval_reviews_6_(2).txt'\n",
        "#path_eval_3 = '/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/eval_reviews_6_(3).txt'\n",
        "\n",
        "#path_nouns = '/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/full_nouns_list.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClojRocSuwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_list = get_lines(path_full)\n",
        "nouns_dict = get_nouns(full_list)\n",
        "nouns_list = get_sorted_list(nouns_dict)\n",
        "#write_nouns(nouns_list, path_nouns)\n",
        "\n",
        "frequent_nouns = nouns_list[:int(round(len(nouns_list) / 10))] #Get top 10% most frequent nouns\n",
        "print(\"Frequent nouns (top 10%) (look at first 30): \", frequent_nouns[:30])\n",
        "\n",
        "#Manually selected test nouns (from frequent_nouns)\n",
        "test_nouns = ['food','service','place','staff','time','customer','atmosphere','pizza','restaurant','chicken'] #for Yelp dataset\n",
        "#test_nouns = ['book','product','time','price','quality','money','game','story','movie','phone'] #for Amazon dataset\n",
        "#test_nouns_news = ['trump','photos','video','world','women','life','kids','people','week','obama'] #for news headlines dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH_YDuziONVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_list = get_sent_lines(path_pos, 'positive')\n",
        "neg_list = get_sent_lines(path_neg, 'negative')\n",
        "\n",
        "final_test_lines_1, final_test_lines_2, final_test_lines_3 = get_test_lines(pos_list, neg_list, test_nouns)\n",
        "print(final_test_lines_1)\n",
        "print(final_test_lines_2)\n",
        "print(final_test_lines_3)\n",
        "#final_news_lines_1, final_news_lines_2, final_news_lines_3 = get_news_lines(full_list, test_nouns_news)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eklcTghuOPeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_eval_lines(final_test_lines_1, path_eval_1)\n",
        "write_eval_lines(final_test_lines_2, path_eval_2)\n",
        "write_eval_lines(final_test_lines_3, path_eval_3)\n",
        "\n",
        "#write_news_lines(final_news_lines_1, path_eval_1)\n",
        "#write_news_lines(final_news_lines_2, path_eval_2)\n",
        "#write_news_lines(final_news_lines_3, path_eval_3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}