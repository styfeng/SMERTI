{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Model_Evaluation (new).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bINI1ZTF1Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NutXatCFHyz",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxypV0XEkkzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "!pip3 install vaderSentiment\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHMO13j7FDdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import string\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "def sentiment_analyzer_scores(sentence):\n",
        "  score = analyser.polarity_scores(sentence)\n",
        "  #print(\"{:-<40} {}\".format(sentence, str(score)))\n",
        "  return score\n",
        "\n",
        "\n",
        "def sentiment_helper(line_sent):\n",
        "  if line_sent[\"compound\"] >= 0.05:\n",
        "    sentiment = \"positive\"\n",
        "  elif line_sent[\"compound\"] <= -0.05:\n",
        "    sentiment = \"negative\"\n",
        "  else:\n",
        "    sentiment = \"neutral\"\n",
        "  return sentiment\n",
        "\n",
        "    \n",
        "def vader_sentiments(input_path, output_path, total_lines):\n",
        "  input_sentiments_lst = []\n",
        "  output_sentiments_lst = []\n",
        "  total_sent_count = 0\n",
        "  input_lines = [line.strip().split('\\t') for line in open(input_path)]\n",
        "  output_lines = [line.strip().split('\\t') for line in open(output_path)]\n",
        "  \n",
        "  for input_line, output_line in zip(input_lines, output_lines):\n",
        "    input_line_sent = sentiment_analyzer_scores(input_line[1])\n",
        "    output_line_sent = sentiment_analyzer_scores(output_line[0])\n",
        "    \n",
        "    input_sent = sentiment_helper(input_line_sent)\n",
        "    output_sent = sentiment_helper(output_line_sent)\n",
        "\n",
        "    if input_sent == output_sent:\n",
        "      total_sent_count += 1\n",
        "      \n",
        "    input_sentiments_lst.append(input_sent)\n",
        "    output_sentiments_lst.append(output_sent)\n",
        "  \n",
        "  final_sent_score = total_sent_count / total_lines\n",
        "  print(\"Final sentiment score: \", final_sent_score)\n",
        "  \n",
        "  return input_sentiments_lst, output_sentiments_lst, total_sent_count, final_sent_score\n",
        "\n",
        "\n",
        "total_lines = 1000\n",
        "\n",
        "input_path = \"/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/eval_reviews_6_(3).txt\"\n",
        "output_path = \"/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/yelp_output_80_seq2seq_new_(3).txt\"\n",
        "\n",
        "input_sent_lst, output_sent_lst, total_sent_count, final_sent_score = vader_sentiments(input_path, output_path, total_lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hU_g7TFI_T",
        "colab_type": "text"
      },
      "source": [
        "#BLEU Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRrR4pb2FJNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def calc_corpus_BLEU(references, hypotheses):\n",
        "  BLEU_score = corpus_bleu(references, hypotheses)\n",
        "  print(\"Corpus BLEU score: \", BLEU_score)\n",
        "  return BLEU_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1AUiRWDQckC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_BLEU(input_path, output_path):\n",
        "  references = [[(line.strip().split('\\t'))[1].split()] for line in open(input_path)]\n",
        "  hypotheses = [(line.strip().split('\\t'))[0].split() for line in open(output_path)]\n",
        "  corpus_BLEU_score = calc_corpus_BLEU(references, hypotheses)\n",
        "  return corpus_BLEU_score\n",
        "\n",
        "\n",
        "input_path = \"/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/eval_reviews_6_(3).txt\"\n",
        "output_path = \"/content/drive/My Drive/Yelp_Dataset/data/evaluation_data/yelp_output_20_seq2seq_new_(3).txt\"\n",
        "\n",
        "corpus_BLEU_score = get_corpus_BLEU(input_path, output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdRA5-QjkXVe",
        "colab_type": "text"
      },
      "source": [
        "#Perplexity & SLOR Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROh6qyv8kg5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q27ayewIkWMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import FlairEmbeddings\n",
        "import math\n",
        "import io\n",
        "\n",
        "# get language model\n",
        "language_model = FlairEmbeddings('news-forward').lm\n",
        "\n",
        "\n",
        "def calc_perplexity(sentence):\n",
        "  if len(sentence) == 1:\n",
        "    sentence_perplexity = language_model.calculate_perplexity(sentence + ' ')\n",
        "  else:\n",
        "    sentence_perplexity = language_model.calculate_perplexity(sentence)\n",
        "  #print(f'\"{sentence}\" - perplexity is {sentence_perplexity}')\n",
        "  return sentence_perplexity\n",
        "\n",
        "\n",
        "def calc_token_perplexities(token_lst):\n",
        "  total_token_counter = 0\n",
        "  token_perplexity_sum = 0\n",
        "  for token in token_lst:\n",
        "    if len(token) == 1:\n",
        "      token_len = 2 \n",
        "      token_perplexity = calc_perplexity(token + ' ')\n",
        "    else:\n",
        "      token_len = len(token)\n",
        "      token_perplexity = calc_perplexity(token)\n",
        "    total_token_counter += token_len\n",
        "    token_perplexity_sum += token_len*math.log(token_perplexity)\n",
        "  return token_perplexity_sum, total_token_counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khEG9kNNFKmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_metrics(path, total_lines, file_type):\n",
        "  print(\"Currently calculating metrics for file: \", path)\n",
        "  \n",
        "  total_masking = 0\n",
        "  avg_masking = 0\n",
        "  total_input_perplexity = 0\n",
        "  avg_input_perplexity = 0\n",
        "  total_input_SLOR = 0\n",
        "  avg_input_SLOR = 0\n",
        "  total_output_perplexity = 0\n",
        "  avg_output_perplexity = 0\n",
        "  total_output_SLOR = 0\n",
        "  avg_output_SLOR = 0\n",
        "  \n",
        "  if file_type == \"input\":\n",
        "    input_lines = [line.strip().split('\\t') for line in open(path)]\n",
        "  \n",
        "    for input_line in input_lines:\n",
        "      if len(input_line[1]) == 0 or len(input_line[1].split()) == 0:\n",
        "        print(\"Error input line: \", input_line)\n",
        "      \n",
        "      else:\n",
        "        input_perplexity = calc_perplexity(input_line[1])\n",
        "        total_input_perplexity += input_perplexity\n",
        "\n",
        "        input_tokens = input_line[1].split()\n",
        "        input_tokens_perplexity, tokens_len = calc_token_perplexities(input_tokens)\n",
        "        input_SLOR = -math.log(input_perplexity) + input_tokens_perplexity / tokens_len\n",
        "        total_input_SLOR += input_SLOR\n",
        "\n",
        "    avg_input_perplexity = total_input_perplexity / total_lines\n",
        "    print(\"Avg input perplexity: \", avg_input_perplexity)\n",
        "\n",
        "    avg_input_SLOR = total_input_SLOR / total_lines\n",
        "    print(\"Avg input SLOR: \", avg_input_SLOR)\n",
        "\n",
        "  else:\n",
        "    output_lines = [line.strip().split('\\t') for line in open(path)]\n",
        "      \n",
        "    for output_line in output_lines:\n",
        "      if len(output_line) < 3 or len(output_line[0]) == 0 or len(output_line[0].split()) == 0:\n",
        "      #Below line is if condition for news headlines dataset\n",
        "      #if len(output_line) < 2 or len(output_line[0]) == 0 or len(output_line[0].split()) == 0:\n",
        "        print(\"Error output line: \", output_line)\n",
        "      \n",
        "      else:\n",
        "        total_masking += float(output_line[1])\n",
        "\n",
        "        output_perplexity = calc_perplexity(output_line[0])\n",
        "        total_output_perplexity += output_perplexity\n",
        "\n",
        "        output_tokens = output_line[0].split()\n",
        "        output_tokens_perplexity, tokens_len = calc_token_perplexities(output_tokens)\n",
        "        output_SLOR = -math.log(output_perplexity) + output_tokens_perplexity / tokens_len\n",
        "        total_output_SLOR += output_SLOR\n",
        "\n",
        "    avg_masking = total_masking / total_lines\n",
        "    print(\"Avg masking: \", avg_masking)\n",
        "\n",
        "    avg_output_perplexity = total_output_perplexity / total_lines\n",
        "    print(\"Avg output perplexity: \", avg_output_perplexity)\n",
        "\n",
        "    avg_output_SLOR = total_output_SLOR / total_lines\n",
        "    print(\"Avg output SLOR: \", avg_output_SLOR)\n",
        "\n",
        "  return avg_masking, avg_input_perplexity, avg_input_SLOR, avg_output_perplexity, avg_output_SLOR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBSTSjld5nhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_SLOR_lst(path, file_type):\n",
        "  print(\"Currently getting SLOR scores for file: \", path)\n",
        "  SLOR_lst = []\n",
        "  \n",
        "  if file_type == \"input\":\n",
        "    input_lines = [line.strip().split('\\t') for line in open(path)]\n",
        " \n",
        "    for input_line in input_lines:\n",
        "      input_perplexity = calc_perplexity(input_line[1])\n",
        "      input_tokens = input_line[1].split()\n",
        "      input_tokens_perplexity, tokens_len = calc_token_perplexities(input_tokens)\n",
        "      input_SLOR = -math.log(input_perplexity) + input_tokens_perplexity / tokens_len  \n",
        "      SLOR_lst.append(input_SLOR)\n",
        "    \n",
        "  else:\n",
        "    output_lines = [line.strip().split('\\t') for line in open(path)]\n",
        "      \n",
        "    for output_line in output_lines:\n",
        "      output_perplexity = calc_perplexity(output_line[0])\n",
        "      output_tokens = output_line[0].split()\n",
        "      output_tokens_perplexity, tokens_len = calc_token_perplexities(output_tokens)\n",
        "      output_SLOR = -math.log(output_perplexity) + output_tokens_perplexity / tokens_len\n",
        "      SLOR_lst.append(output_SLOR)\n",
        "\n",
        "  return SLOR_lst\n",
        "\n",
        "\n",
        "def write_output_lst(lst, path):\n",
        "  f = io.open(path, \"w\", encoding = 'utf-8')\n",
        "  print(\"Currently writing lines to file ...\")\n",
        "  f.write('\\n'.join(str(round(x, 10)) for x in lst))\n",
        "  f.close()\n",
        "  print(\"Lines successfully written to file!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}